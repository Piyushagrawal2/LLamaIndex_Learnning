{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f3b57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n"
     ]
    }
   ],
   "source": [
    "#core LlamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "#LLM Integrations\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "#Embedding Integrations\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "#utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"✅ Imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70b3832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google GenAI API key loaded (starts with: AIzaSyBk...)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is set\n",
    "google_genai_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_genai_api_key:\n",
    "    raise ValueError(\n",
    "        \"❌ GOOGLE_API_KEY not found!\\n\"\n",
    "        \"Please create a .env file in the project root with:\\n\"\n",
    "        \"GOOGLE_API_KEY=your_key_here\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Google GenAI API key loaded (starts with: {google_genai_api_key[:8]}...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb935697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global Settings configured successfully!\n",
      "   LLM: gemini-2.5-flash\n",
      "   Embedding: text-embedding-004\n",
      "   Chunk size: 1024 tokens\n",
      "   Chunk overlap: 200 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM\n",
    "# from llama_index.llms.google_genai.base import GoogleGenAI\n",
    "Settings.llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",  # Fast, cost-effective for most use cases\n",
    "    temperature=0.1,      # Low temperature for consistent responses\n",
    ")\n",
    "\n",
    "# Configure Embedding Model\n",
    "Settings.embed_model = GoogleGenAIEmbedding(\n",
    "    model=\"gemini-embedding-001\",  # 1536 dimensions, good balance\n",
    "    dimensions=1536,                 # Can be reduced for speed (e.g., 512)\n",
    ")\n",
    "\n",
    "# Configure Text Chunking\n",
    "Settings.chunk_size = 1024           # Tokens per chunk (typical: 512-1024)\n",
    "Settings.chunk_overlap = 200         # 20% overlap helps preserve context\n",
    "\n",
    "# Configure Node Parser\n",
    "Settings.node_parser = SentenceSplitter(\n",
    "    chunk_size=Settings.chunk_size,\n",
    "    chunk_overlap=Settings.chunk_overlap,\n",
    ")\n",
    "\n",
    "print(\"✅ Global Settings configured successfully!\")\n",
    "print(f\"   LLM: {Settings.llm.model}\")\n",
    "print(f\"   Embedding: {Settings.embed_model.model_name}\")\n",
    "print(f\"   Chunk size: {Settings.chunk_size} tokens\")\n",
    "print(f\"   Chunk overlap: {Settings.chunk_overlap} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49695cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 3 sample documents\n",
      "   Total characters: 1169\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents (in practice, load from files)\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        LlamaIndex is a data framework for large language models (LLMs). \n",
    "        It provides tools to ingest, structure, and access private or domain-specific data.\n",
    "        LlamaIndex was created to solve the problem of connecting LLMs to external data sources.\n",
    "        The framework supports various data sources including PDFs, databases, APIs, and web pages.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"intro\", \"category\": \"overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Vector embeddings are numerical representations of text that capture semantic meaning.\n",
    "        In LlamaIndex, embeddings enable semantic search - finding relevant content based on meaning,\n",
    "        not just keyword matching. The default embedding model is OpenAI's text-embedding-3-small,\n",
    "        which produces 1536-dimensional vectors. Other models like all-MiniLM-L6-v2 produce 384 dimensions.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"embeddings\", \"category\": \"technical\"}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        The VectorStoreIndex is the most common index type in LlamaIndex. It stores document embeddings\n",
    "        in a vector database and performs similarity search during queries. When you query the index,\n",
    "        it retrieves the most semantically similar chunks and passes them to the LLM as context.\n",
    "        This is the foundation of Retrieval-Augmented Generation (RAG).\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vector_index\", \"category\": \"technical\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(documents)} sample documents\")\n",
    "print(f\"   Total characters: {sum(len(doc.text) for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4b56e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex...\n",
      "This will:\n",
      "  1. Chunk documents into nodes\n",
      "  2. Generate embeddings for each node\n",
      "  3. Store in in-memory vector store\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83399d80ce0e4ba1a137634e49118b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a28f27c489241908f0e2c40ffecdd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Create index from documents\n",
    "print(\"Creating VectorStoreIndex...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. Chunk documents into nodes\")\n",
    "print(\"  2. Generate embeddings for each node\")\n",
    "print(\"  3. Store in in-memory vector store\\n\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,  #displays the progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Index created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1837a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query engine created!\n",
      "   Top-K: 2\n",
      "   Response mode: compact\n"
     ]
    }
   ],
   "source": [
    "# Create query engine from index\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2,  # Retrieve top 2 most similar chunks\n",
    "    response_mode=\"compact\",  # Compact response synthesis\n",
    ")\n",
    "\n",
    "print(\"✅ Query engine created!\")\n",
    "print(f\"   Top-K: {2}\")\n",
    "print(f\"   Response mode: compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34bcb6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is LlamaIndex used for?\n",
      "\n",
      "Response:\n",
      "--------------------------------------------------------------------------------\n",
      "LlamaIndex is a data framework designed for large language models (LLMs). It offers tools to ingest, structure, and access private or domain-specific data. Its primary purpose is to connect LLMs to external data sources.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the index\n",
    "query = \"What is LlamaIndex used for?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(\"-\" * 80)\n",
    "print(response)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831981dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do embeddings work in LlamaIndex?\n",
      "\n",
      "Response:\n",
      "In LlamaIndex, embeddings are numerical representations of text that capture semantic meaning. They facilitate semantic search, allowing for the discovery of relevant content based on its meaning rather than just keyword matches.\n",
      "\n",
      "The VectorStoreIndex stores these document embeddings in a vector database. When a query is made, the index performs a similarity search using these embeddings to identify and retrieve the most semantically similar chunks of information. These retrieved chunks are then passed to a Large Language Model (LLM) as context, which is the underlying mechanism for Retrieval-Augmented Generation (RAG). The default model for generating these embeddings is OpenAI's text-embedding-3-small, which creates 1536-dimensional vectors, though other models like all-MiniLM-L6-v2, producing 384-dimensional vectors, can also be used.\n",
      "\n",
      "Top retrieved source:\n",
      "  Category: technical\n",
      "  Score: 0.7761\n"
     ]
    }
   ],
   "source": [
    "query1 = \"How do embeddings work in LlamaIndex?\"\n",
    "response1 = query_engine.query(query1)\n",
    "\n",
    "print(f\"Query: {query1}\\n\")\n",
    "print(\"Response:\")\n",
    "print(response1)\n",
    "print(\"\\nTop retrieved source:\")\n",
    "print(f\"  Category: {response1.source_nodes[0].metadata.get('category')}\")\n",
    "print(f\"  Score: {response1.source_nodes[0].score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a31e55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Understanding the Document → Node → Index Flow\n",
    "\n",
    "### Inspecting Nodes Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a16b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes created: 3\n",
      "\n",
      "Node 1:\n",
      "  ID: 360dacd5-ebe0-4895-9f43-97205409a815\n",
      "  Text length: 354 characters\n",
      "  Metadata: {'source': 'intro', 'category': 'overview'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='51b7aa84-8584-4552-84f5-fb5999d7d4d0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'intro', 'category': 'overview'}, hash='6b67115e521a90d22245235f7a97b2808451a01b637bc146fc4c6b1b0126d392')}\n",
      "\n",
      "Node 2:\n",
      "  ID: b93dfcde-8fc6-458b-9d6a-3f9d037ebf02\n",
      "  Text length: 395 characters\n",
      "  Metadata: {'source': 'embeddings', 'category': 'technical'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='30c5fa28-61f3-4757-91d1-ac58b0e89aa5', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'embeddings', 'category': 'technical'}, hash='c74e00722500b0b510e568d1fd63750c229430501577b8d0a9295eaa48b99fb3')}\n",
      "\n",
      "Node 3:\n",
      "  ID: 6c0366b1-bad2-4ccd-8cb3-ced9a5a9c868\n",
      "  Text length: 366 characters\n",
      "  Metadata: {'source': 'vector_index', 'category': 'technical'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cfb7ffea-9d93-4483-a662-9de668570131', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'vector_index', 'category': 'technical'}, hash='e70f293d1f0d5f862e85b3f38f9fcdd444e6238533b3c87fb9d78272b4a683bd')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"Number of nodes created: {len(nodes)}\\n\")\n",
    "\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Text length: {len(node.text)} characters\")\n",
    "    print(f\"  Metadata: {node.metadata}\")\n",
    "    print(f\"  Relationships: {node.relationships}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6a7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-K = 1:\n",
      "  Retrieved 1 nodes\n",
      "  Response length: 405 characters\n",
      "  First source score: 0.7174\n",
      "\n",
      "Top-K = 2:\n",
      "  Retrieved 2 nodes\n",
      "  Response length: 608 characters\n",
      "  First source score: 0.7174\n",
      "\n",
      "Top-K = 3:\n",
      "  Retrieved 3 nodes\n",
      "  Response length: 418 characters\n",
      "  First source score: 0.7174\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Explain vectore embeddings\"\n",
    "\n",
    "for k in [1,2,3]:\n",
    "    engine = index.as_query_engine(similarity_top_k=k)\n",
    "    response = engine.query(test_query)\n",
    "\n",
    "    print(f\"\\nTop-K = {k}:\")\n",
    "    print(f\"  Retrieved {len(response.source_nodes)} nodes\")\n",
    "    print(f\"  Response length: {len(str(response))} characters\")\n",
    "    print(f\"  First source score: {response.source_nodes[0].score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e09ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode: compact\n",
      "Response: LlamaIndex is a data framework for large language models (LLMs) that provides tools to ingest, structure, and access private or domain-specific data. It was developed to connect LLMs with external data sources, supporting various types such as PDFs, databases, APIs, and web pages. A key component is the VectorStoreIndex, which stores document embeddings in a vector database and performs similarity searches during queries. This process retrieves semantically similar data chunks and passes them to the LLM as context, forming the foundation of Retrieval-Augmented Generation (RAG).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: tree_summarize\n",
      "Response: LlamaIndex is a data framework designed for large language models (LLMs). Its primary function is to provide tools for ingesting, structuring, and accessing private or domain-specific data, thereby connecting LLMs to external data sources. It supports a variety of data sources, such as PDFs, databases, APIs, and web pages. A key component is the VectorStoreIndex, which stores document embeddings in a vector database and uses similarity search to retrieve semantically similar information to provide as context to the LLM, forming the basis of Retrieval-Augmented Generation (RAG).\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: simple_summarize\n",
      "Response: LlamaIndex is a data framework designed for large language models (LLMs). It offers tools to ingest, structure, and access private or domain-specific data, addressing the challenge of connecting LLMs to external information. The framework supports a variety of data sources, such as PDFs, databases, APIs, and web pages. A common index type within LlamaIndex, the VectorStoreIndex, stores document embeddings in a vector database and facilitates similarity searches during queries. This process retrieves semantically similar data chunks, which are then provided to the LLM as context, forming the basis of Retrieval-Augmented Generation (RAG).\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test different response modes\n",
    "modes = [\"compact\", \"tree_summarize\", \"simple_summarize\"]\n",
    "test_query = \"What are the key features of LlamaIndex?\"\n",
    "\n",
    "for mode in modes:\n",
    "    engine = index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        response_mode=mode\n",
    "    )\n",
    "    response = engine.query(test_query)\n",
    "    \n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bf8fb",
   "metadata": {},
   "source": [
    "### Response Mode Comparison\n",
    "\n",
    "| Mode | How It Works | Best For |\n",
    "|------|-------------|----------|\n",
    "| **compact** | Concatenates chunks, refines iteratively | Balanced quality/speed |\n",
    "| **tree_summarize** | Builds summary tree hierarchically | Large context, comprehensive answers |\n",
    "| **simple_summarize** | Concatenates all chunks, single LLM call | Simple queries, speed |\n",
    "| **refine** | Iteratively refines answer with each chunk | High quality, slower |\n",
    "| **accumulate** | Generates separate answer per chunk | Multiple perspectives |\n",
    "\n",
    "**Default**: `compact` (good balance for most use cases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
