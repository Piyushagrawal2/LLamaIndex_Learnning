{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691fbe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "#Node parser (Chunking strategies)\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    TokenTextSplitter,\n",
    "    SemanticSplitterNodeParser\n",
    ")\n",
    "\n",
    "#metadata extractions\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    SummaryExtractor\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "#LLm and Embaddings\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "\n",
    "#utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e44387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Settings configured\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables and configure Settings\n",
    "from llama_index.llms.google_genai.base import GoogleGenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "Settings.llm = GoogleGenAI(model=\"gemini-3-flash-preview\", temperature=0.1)\n",
    "Settings.embed_model = GoogleGenAIEmbedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    dimensions=1536\n",
    ")\n",
    "\n",
    "print(\"âœ… Settings configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcc942",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading Documents from Multiple Sources\n",
    "\n",
    "### 2.1 Local File Loading with SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b44e4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory exists: True\n",
      "Sample docs directory: True\n",
      "Research papers directory: True\n",
      "\n",
      "Found 1 PDF files in research_papers/\n",
      "  - Piyush_Agrawal_AI_resume.pdf\n"
     ]
    }
   ],
   "source": [
    "# Check data directory structure\n",
    "data_dir = Path(\"./data\")\n",
    "sample_docs_dir = data_dir / \"sample_docs\"\n",
    "research_papers_dir = data_dir / \"research_papers\"\n",
    "\n",
    "print(f\"Data directory exists: {data_dir.exists()}\")\n",
    "print(f\"Sample docs directory: {sample_docs_dir.exists()}\")\n",
    "print(f\"Research papers directory: {research_papers_dir.exists()}\")\n",
    "\n",
    "if research_papers_dir.exists():\n",
    "    files = list(research_papers_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\nFound {len(files)} PDF files in research_papers/\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39443acb",
   "metadata": {},
   "source": [
    "### SimpleDirectoryReader Features\n",
    "\n",
    "**Key Parameters:**\n",
    "- `input_dir`: Directory path\n",
    "- `required_exts`: Filter by extensions (e.g., `[\".pdf\", \".txt\"]`)\n",
    "- `recursive`: Scan subdirectories\n",
    "- `filename_as_id`: Use filename as document ID\n",
    "- `file_metadata`: Custom metadata function\n",
    "- `exclude_hidden`: Skip hidden files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a070cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 3 sample research papers\n",
      "  - Attention Is All You Need (2017)\n",
      "  - BERT (2019)\n",
      "  - RAG (2020)\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents if no PDFs available\n",
    "# In practice, you'd load actual PDFs from the data directory\n",
    "\n",
    "sample_papers = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: Attention Is All You Need\n",
    "        Authors: Vaswani et al.\n",
    "        Year: 2017\n",
    "        \n",
    "        Abstract: The dominant sequence transduction models are based on complex recurrent or \n",
    "        convolutional neural networks that include an encoder and a decoder. The best performing \n",
    "        models also connect the encoder and decoder through an attention mechanism. We propose a \n",
    "        new simple network architecture, the Transformer, based solely on attention mechanisms, \n",
    "        dispensing with recurrence and convolutions entirely.\n",
    "        \n",
    "        Introduction: Recurrent neural networks, long short-term memory and gated recurrent neural \n",
    "        networks in particular, have been firmly established as state of the art approaches in \n",
    "        sequence modeling and transduction problems. The Transformer is the first transduction model \n",
    "        relying entirely on self-attention to compute representations of its input and output without \n",
    "        using sequence-aligned RNNs or convolution.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"Attention Is All You Need\",\n",
    "            \"authors\": \"Vaswani et al.\",\n",
    "            \"year\": 2017,\n",
    "            \"category\": \"transformers\",\n",
    "            \"citations\": 85000,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "        Authors: Devlin et al.\n",
    "        Year: 2019\n",
    "        \n",
    "        Abstract: We introduce a new language representation model called BERT, which stands for \n",
    "        Bidirectional Encoder Representations from Transformers. Unlike recent language representation \n",
    "        models, BERT is designed to pre-train deep bidirectional representations from unlabeled text \n",
    "        by jointly conditioning on both left and right context in all layers.\n",
    "        \n",
    "        Introduction: Language model pre-training has been shown to be effective for improving many \n",
    "        natural language processing tasks. Pre-trained language representations can be either context-free \n",
    "        or context-based. BERT alleviates the unidirectionality constraint by using a masked language \n",
    "        model (MLM) pre-training objective.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"BERT\",\n",
    "            \"authors\": \"Devlin et al.\",\n",
    "            \"year\": 2019,\n",
    "            \"category\": \"language_models\",\n",
    "            \"citations\": 65000,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "        Authors: Lewis et al.\n",
    "        Year: 2020\n",
    "        \n",
    "        Abstract: Large pre-trained language models have been shown to store factual knowledge in their \n",
    "        parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, \n",
    "        their ability to access and precisely manipulate knowledge is still limited. We explore a general \n",
    "        fine-tuning recipe for retrieval-augmented generation (RAG) models which combine parametric and \n",
    "        non-parametric memory.\n",
    "        \n",
    "        Introduction: Pre-trained neural language models store and retrieve knowledge using their parameters. \n",
    "        RAG models combine parametric memory (the LLM) with non-parametric memory (a dense vector index of \n",
    "        Wikipedia). This provides the model with access to up-to-date information and allows for more \n",
    "        interpretable and modular systems.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"RAG\",\n",
    "            \"authors\": \"Lewis et al.\",\n",
    "            \"year\": 2020,\n",
    "            \"category\": \"rag\",\n",
    "            \"citations\": 3500,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(sample_papers)} sample research papers\")\n",
    "for doc in sample_papers:\n",
    "    print(f\"  - {doc.metadata['title']} ({doc.metadata['year']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf336a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced metadata for first document:\n",
      "title: Attention Is All You Need\n",
      "authors: Vaswani et al.\n",
      "year: 2017\n",
      "category: transformers\n",
      "citations: 85000\n",
      "source: research_paper\n",
      "processed_date: 2026-01-11T20:14:06.403379\n",
      "char_count: 1006\n",
      "word_count: 123\n"
     ]
    }
   ],
   "source": [
    "#Add processsing metadata\n",
    "for doc in sample_papers:\n",
    "    doc.metadata[\"processed_date\"] = datetime.now().isoformat()\n",
    "    doc.metadata[\"char_count\"] = len(doc.text)\n",
    "    doc.metadata[\"word_count\"] = len(doc.text.split())\n",
    "\n",
    "print(\"Enhanced metadata for first document:\")\n",
    "for key, value in sample_papers[0].metadata.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d607cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Chunking Strategies\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "Chunking is **critical** for RAG quality:\n",
    "\n",
    "1. **Context Window Limits**: LLMs have token limits\n",
    "2. **Embedding Quality**: Smaller chunks = more focused embeddings\n",
    "3. **Retrieval Precision**: Granular chunks improve relevance\n",
    "4. **Cost Optimization**: Smaller chunks = fewer tokens to LLM\n",
    "\n",
    "### 3.1 Sentence-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3cb67cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceSplitter Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 3\n",
      "  Avg chars per node: 936\n",
      "\n",
      "First node preview:\n",
      "  Text (first 200 chars): Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are based on complex recurrent or \n",
      "        convolutiona...\n",
      "  Metadata: {'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2026-01-11T20:14:06.403379', 'char_count': 1006, 'word_count': 123}\n"
     ]
    }
   ],
   "source": [
    "# SentenceSplitter: Respects sentence boundaries\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,     # Target tokens per chunk\n",
    "    chunk_overlap=200,   # Overlap to preserve context\n",
    "    separator=\" \",       # Split on spaces first\n",
    ")\n",
    "\n",
    "sentence_nodes = sentence_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"SentenceSplitter Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(sentence_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in sentence_nodes) / len(sentence_nodes):.0f}\")\n",
    "\n",
    "print(f\"\\nFirst node preview:\")\n",
    "print(f\"  Text (first 200 chars): {sentence_nodes[0].text[:200]}...\")\n",
    "print(f\"  Metadata: {sentence_nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f94bd3",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ ML Engineering Note: Chunk Size Selection\n",
    "\n",
    "**Chunk Size Trade-offs:**\n",
    "\n",
    "| Size | Pros | Cons | Use Case |\n",
    "|------|------|------|----------|\n",
    "| **Small (256-512)** | Precise retrieval, lower cost | May lose context | Q&A, factoid extraction |\n",
    "| **Medium (512-1024)** | Balanced context/precision | Good default | General RAG, document QA |\n",
    "| **Large (1024-2048)** | Rich context | Diluted relevance, higher cost | Summarization, broad queries |\n",
    "\n",
    "**Overlap Guidelines:**\n",
    "- 10-20% of chunk size (typical)\n",
    "- Higher overlap (20-30%) for dense, technical content\n",
    "- Lower overlap (5-10%) for structured documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01a6f8",
   "metadata": {},
   "source": [
    "### 3.2 Token-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b178dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTextSplitter Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 3\n",
      "  Avg chars per node: 936\n",
      "\n",
      "Comparison:\n",
      "  SentenceSplitter: 3 nodes\n",
      "  TokenTextSplitter: 3 nodes\n",
      "  Difference: 0 nodes\n"
     ]
    }
   ],
   "source": [
    "#TokenTextSplitter: Precise token count control\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    "    separator=\" \",\n",
    ")\n",
    "\n",
    "token_nodes = token_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"TokenTextSplitter Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(token_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in token_nodes) / len(token_nodes):.0f}\")\n",
    "\n",
    "# Compare with sentence splitter\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  SentenceSplitter: {len(sentence_nodes)} nodes\")\n",
    "print(f\"  TokenTextSplitter: {len(token_nodes)} nodes\")\n",
    "print(f\"  Difference: {abs(len(sentence_nodes) - len(token_nodes))} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e722426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First node preview:\n",
      "  Text (first 200 chars): Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "        Authors: Devlin et al.\n",
      "        Year: 2019\n",
      "\n",
      "        Abstract: We introduce a new language representation ...\n",
      "  Metadata: {'title': 'BERT', 'authors': 'Devlin et al.', 'year': 2019, 'category': 'language_models', 'citations': 65000, 'source': 'research_paper', 'processed_date': '2026-01-11T20:14:06.403405', 'char_count': 895, 'word_count': 102}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFirst node preview:\")\n",
    "print(f\"  Text (first 200 chars): {token_nodes[1].text[:200]}...\")\n",
    "print(f\"  Metadata: {token_nodes[1].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2781cf",
   "metadata": {},
   "source": [
    "### 3.3 Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a2993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating semantic chunks (this will call embedding API)...\n",
      "\n",
      "SemanticSplitterNodeParser Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 6\n",
      "  Avg chars per node: 477\n",
      "  Min chars: 220\n",
      "  Max chars: 740\n"
     ]
    }
   ],
   "source": [
    "# SemanticSplitterNodeParser: Chunk by meaning, not just size\n",
    "sementic_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=Settings.embed_model,\n",
    ")\n",
    "\n",
    "print(\"Creating semantic chunks (this will call embedding API)...\")\n",
    "semantic_nodes = sementic_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"\\nSemanticSplitterNodeParser Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(semantic_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in semantic_nodes) / len(semantic_nodes):.0f}\")\n",
    "print(f\"  Min chars: {min(len(n.text) for n in semantic_nodes)}\")\n",
    "print(f\"  Max chars: {max(len(n.text) for n in semantic_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84504ba",
   "metadata": {},
   "source": [
    "### 3.4 Comparing Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e24dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking Strategy Comparison:\n",
      "Strategy  Num Nodes  Avg Chars  Min Chars  Max Chars  Std Dev\n",
      "Sentence          3        936        877        988       55\n",
      "   Token          3        936        877        988       55\n",
      "Semantic          6        477        220        740      228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compare chunking strategies\n",
    "strategies = [\n",
    "    {\"name\": \"Sentence\", \"nodes\": sentence_nodes},\n",
    "    {\"name\": \"Token\", \"nodes\": token_nodes},\n",
    "    {\"name\": \"Semantic\", \"nodes\": semantic_nodes},\n",
    "]\n",
    "\n",
    "comparison_data = []\n",
    "for strat in strategies:\n",
    "    nodes = strat[\"nodes\"]\n",
    "    comparison_data.append({\n",
    "        \"Strategy\": strat[\"name\"],\n",
    "        \"Num Nodes\": len(nodes),\n",
    "        \"Avg Chars\": int(sum(len(n.text) for n in nodes) / len(nodes)),\n",
    "        \"Min Chars\": min(len(n.text) for n in nodes),\n",
    "        \"Max Chars\": max(len(n.text) for n in nodes),\n",
    "        \"Std Dev\": int(pd.Series([len(n.text) for n in nodes]).std()),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nChunking Strategy Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b9094",
   "metadata": {},
   "source": [
    "### 4.2 LLM-Based Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c5281ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.core.extractors import SummaryExtractor, TitleExtractor\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "async def extract_metadata_async(sentence_nodes):\n",
    "    # Create extractors\n",
    "    title_extractor = TitleExtractor(\n",
    "        llm=Settings.llm,\n",
    "        nodes=5,\n",
    "    )\n",
    "\n",
    "    summary_extractor = SummaryExtractor(\n",
    "        llm=Settings.llm,\n",
    "        summaries=[\"self\"],  # Summarize each node\n",
    "    )\n",
    "\n",
    "    print(\"Extracting metadata with LLM (this may take a moment)...\")\n",
    "\n",
    "    # Apply to a subset of nodes (to save API calls)\n",
    "    sample_nodes_for_extraction = sentence_nodes[:2]\n",
    "\n",
    "    # ðŸ”¥ ASYNC extraction\n",
    "    nodes_with_summaries = await summary_extractor.aprocess_nodes(\n",
    "        sample_nodes_for_extraction\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Extracted summaries for {len(nodes_with_summaries)} nodes\")\n",
    "    print(f\"\\nNode 0 with LLM-generated summary:\")\n",
    "    print(f\"  Original text (first 150 chars): {nodes_with_summaries[0].text[:150]}...\")\n",
    "\n",
    "    if \"section_summary\" in nodes_with_summaries[0].metadata:\n",
    "        print(\n",
    "            f\"  Summary: {nodes_with_summaries[0].metadata['section_summary']}\"\n",
    "        )\n",
    "\n",
    "    return nodes_with_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "519ff3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 4.89002899s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ðŸ”¥ ASYNC title extraction\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nodes_with_title = \u001b[38;5;28;01mawait\u001b[39;00m title_extractor.aprocess_nodes(\n\u001b[32m      3\u001b[39m     sample_nodes_for_extraction\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Extracted summaries for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nodes_with_summaries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nodes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNode 0 with LLM-generated summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/extractors/interface.py:125\u001b[39m, in \u001b[36mBaseExtractor.aprocess_nodes\u001b[39m\u001b[34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    123\u001b[39m     new_nodes = [deepcopy(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m cur_metadata_list = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aextract(new_nodes)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_nodes):\n\u001b[32m    127\u001b[39m     node.metadata.update(cur_metadata_list[idx])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/extractors/metadata_extractors.py:115\u001b[39m, in \u001b[36mTitleExtractor.aextract\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode]) -> List[Dict]:\n\u001b[32m    114\u001b[39m     nodes_by_doc_id = \u001b[38;5;28mself\u001b[39m.separate_nodes_by_ref_id(nodes)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     titles_by_doc_id = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract_titles(nodes_by_doc_id)\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[33m\"\u001b[39m\u001b[33mdocument_title\u001b[39m\u001b[33m\"\u001b[39m: titles_by_doc_id[node.ref_doc_id]} \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/extractors/metadata_extractors.py:155\u001b[39m, in \u001b[36mTitleExtractor.extract_titles\u001b[39m\u001b[34m(self, nodes_by_doc_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, nodes \u001b[38;5;129;01min\u001b[39;00m nodes_by_doc_id.items():\n\u001b[32m    154\u001b[39m     jobs.append(get_titles_by_doc(nodes, key))\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m list_dict_titles: List[Dict] = \u001b[38;5;28;01mawait\u001b[39;00m run_jobs(\n\u001b[32m    156\u001b[39m     jobs=jobs,\n\u001b[32m    157\u001b[39m     show_progress=\u001b[38;5;28mself\u001b[39m.show_progress,\n\u001b[32m    158\u001b[39m )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m list_dict_titles:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m d.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/async_utils.py:170\u001b[39m, in \u001b[36mrun_jobs\u001b[39m\u001b[34m(jobs, show_progress, workers, desc)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masyncio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm_asyncio\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio.gather(*pool_jobs, desc=desc)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    172\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*pool_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tqdm/asyncio.py:79\u001b[39m, in \u001b[36mtqdm_asyncio.gather\u001b[39m\u001b[34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[32m     78\u001b[39m ifs = [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m res = [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[32m     80\u001b[39m                                          total=total, **tqdm_kwargs)]\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:634\u001b[39m, in \u001b[36m_AsCompletedIterator._wait_for_one\u001b[39m\u001b[34m(self, resolve)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;66;03m# Dummy value from _handle_timeout().\u001b[39;00m\n\u001b[32m    633\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m resolve \u001b[38;5;28;01melse\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tqdm/asyncio.py:76\u001b[39m, in \u001b[36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[39m\u001b[34m(i, f)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_awaitable\u001b[39m(i, f):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/async_utils.py:163\u001b[39m, in \u001b[36mrun_jobs.<locals>.worker\u001b[39m\u001b[34m(job)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;129m@dispatcher\u001b[39m.span\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mworker\u001b[39m(job: Coroutine) -> Any:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m job\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/extractors/metadata_extractors.py:145\u001b[39m, in \u001b[36mTitleExtractor.extract_titles.<locals>.get_titles_by_doc\u001b[39m\u001b[34m(nodes, key)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_titles_by_doc\u001b[39m(nodes: List[BaseNode], key: \u001b[38;5;28mstr\u001b[39m) -> Dict:\n\u001b[32m    144\u001b[39m     titles_by_doc_id = {}\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     title_candidates = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_title_candidates(nodes)\n\u001b[32m    146\u001b[39m     combined_titles = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(title_candidates)\n\u001b[32m    147\u001b[39m     titles_by_doc_id[key] = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.apredict(\n\u001b[32m    148\u001b[39m         PromptTemplate(template=\u001b[38;5;28mself\u001b[39m.combine_template),\n\u001b[32m    149\u001b[39m         context_str=combined_titles,\n\u001b[32m    150\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/extractors/metadata_extractors.py:166\u001b[39m, in \u001b[36mTitleExtractor.get_title_candidates\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_title_candidates\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: List[BaseNode]) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.apredict(\n\u001b[32m    167\u001b[39m             PromptTemplate(template=\u001b[38;5;28mself\u001b[39m.node_template),\n\u001b[32m    168\u001b[39m             context_str=cast(TextNode, node).text,\n\u001b[32m    169\u001b[39m         )\n\u001b[32m    170\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[32m    171\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/llms/llm.py:716\u001b[39m, in \u001b[36mLLM.apredict\u001b[39m\u001b[34m(self, prompt, **prompt_args)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    715\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._get_messages(prompt, **prompt_args)\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.achat(messages)\n\u001b[32m    717\u001b[39m     output = chat_response.message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/core/llms/callbacks.py:76\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m     68\u001b[39m     CBEventType.LLM,\n\u001b[32m     69\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     },\n\u001b[32m     74\u001b[39m )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     f_return_val = \u001b[38;5;28;01mawait\u001b[39;00m f(_self, messages, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     78\u001b[39m     callback_manager.on_event_end(\n\u001b[32m     79\u001b[39m         CBEventType.LLM,\n\u001b[32m     80\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m     81\u001b[39m         event_id=event_id,\n\u001b[32m     82\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/llms/google_genai/base.py:355\u001b[39m, in \u001b[36mGoogleGenAI.achat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;129m@llm_chat_callback\u001b[39m()\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34machat\u001b[39m(\n\u001b[32m    353\u001b[39m     \u001b[38;5;28mself\u001b[39m, messages: Sequence[ChatMessage], **kwargs: Any\n\u001b[32m    354\u001b[39m ) -> ChatResponse:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._achat(messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/llama_index/llms/google_genai/base.py:338\u001b[39m, in \u001b[36mGoogleGenAI._achat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m next_msg, chat_kwargs, file_api_names = \u001b[38;5;28;01mawait\u001b[39;00m prepare_chat_params(\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, messages, \u001b[38;5;28mself\u001b[39m.file_mode, \u001b[38;5;28mself\u001b[39m._client, **params\n\u001b[32m    336\u001b[39m )\n\u001b[32m    337\u001b[39m chat = \u001b[38;5;28mself\u001b[39m._client.aio.chats.create(**chat_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m chat.send_message(\n\u001b[32m    339\u001b[39m     next_msg.parts \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_msg, types.Content) \u001b[38;5;28;01melse\u001b[39;00m next_msg\n\u001b[32m    340\u001b[39m )\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfileapi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m adelete_uploaded_files(file_api_names, \u001b[38;5;28mself\u001b[39m._client)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/chats.py:416\u001b[39m, in \u001b[36mAsyncChat.send_message\u001b[39m\u001b[34m(self, message, config)\u001b[39m\n\u001b[32m    411\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    412\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage must be a valid part type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnionDict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    414\u001b[39m   )\n\u001b[32m    415\u001b[39m input_content = t.t_content(message)\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.generate_content(\n\u001b[32m    417\u001b[39m     model=\u001b[38;5;28mself\u001b[39m._model,\n\u001b[32m    418\u001b[39m     contents=\u001b[38;5;28mself\u001b[39m._curated_history + [input_content],  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    419\u001b[39m     config=config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config,\n\u001b[32m    420\u001b[39m )\n\u001b[32m    421\u001b[39m model_output = (\n\u001b[32m    422\u001b[39m     [response.candidates[\u001b[32m0\u001b[39m].content]\n\u001b[32m    423\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.candidates \u001b[38;5;129;01mand\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    425\u001b[39m )\n\u001b[32m    426\u001b[39m automatic_function_calling_history = (\n\u001b[32m    427\u001b[39m     response.automatic_function_calling_history\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.automatic_function_calling_history\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    430\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/models.py:7018\u001b[39m, in \u001b[36mAsyncModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   7016\u001b[39m response = types.GenerateContentResponse()\n\u001b[32m   7017\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7018\u001b[39m   response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_content(\n\u001b[32m   7019\u001b[39m       model=model, contents=contents, config=parsed_config\n\u001b[32m   7020\u001b[39m   )\n\u001b[32m   7021\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n\u001b[32m   7022\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m remaining_remote_calls_afc == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/models.py:5824\u001b[39m, in \u001b[36mAsyncModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5821\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   5822\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m5824\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.async_request(\n\u001b[32m   5825\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m'\u001b[39m, path, request_dict, http_options\n\u001b[32m   5826\u001b[39m )\n\u001b[32m   5828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   5829\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5830\u001b[39m ):\n\u001b[32m   5831\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1434\u001b[39m, in \u001b[36mBaseApiClient.async_request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34masync_request\u001b[39m(\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1425\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1428\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1429\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1430\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1431\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1432\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m   result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_request(\n\u001b[32m   1435\u001b[39m       http_request=http_request, http_options=http_options, stream=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1436\u001b[39m   )\n\u001b[32m   1437\u001b[39m   response_body = result.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m result.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1438\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=result.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1367\u001b[39m, in \u001b[36mBaseApiClient._async_request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1365\u001b[39m     retry = tenacity.AsyncRetrying(**retry_kwargs)\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._async_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_retry(  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28mself\u001b[39m._async_request_once, http_request, stream\n\u001b[32m   1369\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.11/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1312\u001b[39m, in \u001b[36mBaseApiClient._async_request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1304\u001b[39m   response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiohttp_session.request(\n\u001b[32m   1305\u001b[39m       method=http_request.method,\n\u001b[32m   1306\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1310\u001b[39m       **\u001b[38;5;28mself\u001b[39m._async_client_session_request_args,\n\u001b[32m   1311\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1312\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m errors.APIError.raise_for_async_response(response)\n\u001b[32m   1313\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(response.headers, [\u001b[38;5;28;01mawait\u001b[39;00m response.text()])\n\u001b[32m   1314\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m   1315\u001b[39m     aiohttp.ClientConnectorError,\n\u001b[32m   1316\u001b[39m     aiohttp.ClientConnectorDNSError,\n\u001b[32m   1317\u001b[39m     aiohttp.ClientOSError,\n\u001b[32m   1318\u001b[39m     aiohttp.ServerDisconnectedError,\n\u001b[32m   1319\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/errors.py:203\u001b[39m, in \u001b[36mAPIError.raise_for_async_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    200\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnsupported response type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.raise_error_async(status_code, response_json, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/learning/AI Learning/llama_index/venv/lib/python3.13/site-packages/google/genai/errors.py:225\u001b[39m, in \u001b[36mAPIError.raise_error_async\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    212\u001b[39m \n\u001b[32m    213\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    227\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 4.89002899s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}"
     ]
    }
   ],
   "source": [
    "# ðŸ”¥ ASYNC title extraction\n",
    "nodes_with_title = await title_extractor.aprocess_nodes(\n",
    "    sample_nodes_for_extraction\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Extracted summaries for {len(nodes_with_summaries)} nodes\")\n",
    "print(f\"\\nNode 0 with LLM-generated summary:\")\n",
    "print(f\"  Original text (first 150 chars): {nodes_with_summaries[0].text[:150]}...\")\n",
    "\n",
    "# Safely access extracted title\n",
    "if \"title\" in nodes_with_title[0].metadata:\n",
    "    print(f\"Extracted title: {nodes_with_title[0].metadata['title']}\")\n",
    "else:\n",
    "    print(\"No title extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb093f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Attention Is All You Need',\n",
       " 'authors': 'Vaswani et al.',\n",
       " 'year': 2017,\n",
       " 'category': 'transformers',\n",
       " 'citations': 85000,\n",
       " 'source': 'research_paper',\n",
       " 'processed_date': '2026-01-11T20:14:06.403379',\n",
       " 'char_count': 1006,\n",
       " 'word_count': 123,\n",
       " 'section_summary': 'The research paper \"Attention Is All You Need\" by Vaswani et al. (2017) introduces the **Transformer** model, a novel neural network architecture for sequence transduction. The key innovation of the Transformer is its exclusive reliance on **attention mechanisms**, particularly **self-attention**, entirely dispensing with traditional recurrent neural networks (RNNs, LSTMs, GRUs) and convolutional neural networks (CNNs) that were previously dominant. This foundational work in the \"transformers\" category aims to compute representations of input and output without sequence-aligned RNNs or convolutions.',\n",
       " 'document_title': 'Title: Attention Is All You Need (Vaswani et al., 2017): The Transformer, a Pure Attention Model for Sequence Transduction, Dispensing with Recurrence and Convolutions.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_with_summaries[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdf976",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Node Relationships\n",
    "\n",
    "### Understanding Node Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0535891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Relationships:\n",
      "\n",
      "Node 0:\n",
      "  ID: 72d5041b-a9a8-43f4-9f0e-fb836a880973\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: 23aa00b0-7606-437c-b808-8b57b5ca73dd\n",
      "\n",
      "Node 1:\n",
      "  ID: e96f93dc-0d0b-4e29-98eb-8a580aa250f5\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: 2b3f03a5-4719-458c-ba34-47cbd4cb8cb3\n",
      "\n",
      "Node 2:\n",
      "  ID: 0796f718-645f-4b1f-86f2-7534d6161aba\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: eef4b82a-2469-4e10-ba02-70a31656780f\n"
     ]
    }
   ],
   "source": [
    "# Inspect node relationships\n",
    "print(\"Node Relationships:\")\n",
    "for i, node in enumerate(sentence_nodes[:3]):\n",
    "    print(f\"\\nNode {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Relationships: {list(node.relationships.keys())}\")\n",
    "    \n",
    "    # Check for source document\n",
    "    if NodeRelationship.SOURCE in node.relationships:\n",
    "        source_info = node.relationships[NodeRelationship.SOURCE]\n",
    "        print(f\"  Source Document ID: {source_info.node_id}\")\n",
    "    \n",
    "    # Check for previous/next nodes\n",
    "    if NodeRelationship.PREVIOUS in node.relationships:\n",
    "        print(f\"  Has PREVIOUS node\")\n",
    "    if NodeRelationship.NEXT in node.relationships:\n",
    "        print(f\"  Has NEXT node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2b841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Relationships:\n",
      "\n",
      "Node 0:\n",
      "  ID: 6547da38-74e5-4693-9e26-fed7baee5a6a\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.NEXT: '3'>]\n",
      "  Source Document ID: 23aa00b0-7606-437c-b808-8b57b5ca73dd\n",
      "  Has NEXT node\n",
      "\n",
      "Node 1:\n",
      "  ID: f9e401aa-55d2-4cf9-af4b-d55f6174e03b\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.PREVIOUS: '2'>]\n",
      "  Source Document ID: 23aa00b0-7606-437c-b808-8b57b5ca73dd\n",
      "  Has PREVIOUS node\n",
      "\n",
      "Node 2:\n",
      "  ID: 86e44d95-f5a9-4af9-acc3-fd4b3ad528bc\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.NEXT: '3'>]\n",
      "  Source Document ID: 2b3f03a5-4719-458c-ba34-47cbd4cb8cb3\n",
      "  Has NEXT node\n",
      "\n",
      "Node 3:\n",
      "  ID: 67c1d8a3-e458-4882-b7f7-a7ccf65472ce\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.PREVIOUS: '2'>]\n",
      "  Source Document ID: 2b3f03a5-4719-458c-ba34-47cbd4cb8cb3\n",
      "  Has PREVIOUS node\n",
      "\n",
      "Node 4:\n",
      "  ID: a924f165-f25e-4ae8-843f-03b77c5ea97b\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.NEXT: '3'>]\n",
      "  Source Document ID: eef4b82a-2469-4e10-ba02-70a31656780f\n",
      "  Has NEXT node\n",
      "\n",
      "Node 5:\n",
      "  ID: 39dc47d7-9ac4-4d3d-a7a0-dc9e6ac1b049\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.PREVIOUS: '2'>]\n",
      "  Source Document ID: eef4b82a-2469-4e10-ba02-70a31656780f\n",
      "  Has PREVIOUS node\n"
     ]
    }
   ],
   "source": [
    "# Inspect node relationships\n",
    "print(\"Node Relationships:\")\n",
    "for i, node in enumerate(semantic_nodes[:]):\n",
    "    print(f\"\\nNode {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Relationships: {list(node.relationships.keys())}\")\n",
    "    \n",
    "    # Check for source document\n",
    "    if NodeRelationship.SOURCE in node.relationships:\n",
    "        source_info = node.relationships[NodeRelationship.SOURCE]\n",
    "        print(f\"  Source Document ID: {source_info.node_id}\")\n",
    "    \n",
    "    # Check for previous/next nodes\n",
    "    if NodeRelationship.PREVIOUS in node.relationships:\n",
    "        print(f\"  Has PREVIOUS node\")\n",
    "    if NodeRelationship.NEXT in node.relationships:\n",
    "        print(f\"  Has NEXT node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d2036",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Ingestion Pipeline\n",
    "\n",
    "### Creating a Complete Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4777ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ingestion pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4375c48dd646db8640c49fc2240d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a2423a61be4e43ab2f1bb8afa8149d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Pipeline complete!\n",
      "  Processed 3 documents\n",
      "  Generated 3 nodes\n",
      "  Nodes have embeddings: True\n"
     ]
    }
   ],
   "source": [
    "#build ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=200),\n",
    "        Settings.embed_model\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Running ingestion pipeline...\")\n",
    "nodes = pipeline.run(documents=sample_papers, show_progress=True)\n",
    "\n",
    "print(f\"\\nâœ… Pipeline complete!\")\n",
    "print(f\"  Processed {len(sample_papers)} documents\")\n",
    "print(f\"  Generated {len(nodes)} nodes\")\n",
    "print(f\"  Nodes have embeddings: {nodes[0].embedding is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ee09c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Building an Index with Optimized Chunks\n",
    "\n",
    "### Using Our Processed Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cd604a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Index created from processed nodes\n",
      "  Total nodes indexed: 3\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Index created from processed nodes\")\n",
    "print(f\"  Total nodes indexed: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1f563",
   "metadata": {},
   "source": [
    "### Querying with Rich Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0edf9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the Transformer architecture?\n",
      "\n",
      "Response:\n",
      "The Transformer is a network architecture that relies entirely on attention mechanisms to compute representations of its input and output. It was introduced as a simpler model that dispenses with recurrence and convolutions entirely, making it the first transduction model to rely solely on self-attention.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Retrieved Sources:\n",
      "\n",
      "Source 1:\n",
      "  Score: 0.6615\n",
      "  Title: Attention Is All You Need\n",
      "  Year: 2017\n",
      "  Category: transformers\n",
      "  Text preview: Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are b...\n",
      "\n",
      "Source 2:\n",
      "  Score: 0.5488\n",
      "  Title: BERT\n",
      "  Year: 2019\n",
      "  Category: language_models\n",
      "  Text preview: Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "        Authors: Devlin et al.\n",
      "        Year: 2019\n",
      "\n",
      "        Abs...\n",
      "\n",
      "Source 3:\n",
      "  Score: 0.5003\n",
      "  Title: RAG\n",
      "  Year: 2020\n",
      "  Category: rag\n",
      "  Text preview: Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "        Authors: Lewis et al.\n",
      "        Year: 2020\n",
      "\n",
      "        Abstract: Large pre-...\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Ensure this runs first in notebook\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def run_query():\n",
    "    query = \"What is the Transformer architecture?\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Response:\")\n",
    "    print(response.response)  # Use .response for string content\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Examine retrieved sources\n",
    "    print(\"\\nRetrieved Sources:\")\n",
    "    for i, source_node in enumerate(response.source_nodes, 1):\n",
    "        print(f\"\\nSource {i}:\")\n",
    "        print(f\"  Score: {source_node.score:.4f}\")\n",
    "        print(f\"  Title: {source_node.metadata.get('title', 'N/A')}\")\n",
    "        print(f\"  Year: {source_node.metadata.get('year', 'N/A')}\")\n",
    "        print(f\"  Category: {source_node.metadata.get('category', 'N/A')}\")\n",
    "        print(f\"  Text preview: {source_node.text[:150]}...\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run the async function\n",
    "response = asyncio.run(run_query())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff71d14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Chunking Best Practices\n",
    "\n",
    "### Experiment: Impact of Chunk Size on Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02724878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Size Impact on Retrieval:\n",
      " Chunk Size  Num Nodes Top Score  Response Len\n",
      "        256          3    0.6242           227\n",
      "        512          3    0.6242           371\n",
      "       1024          3    0.6242           371\n",
      "       2048          3    0.6242           301\n"
     ]
    }
   ],
   "source": [
    "# Test different chunk sizes\n",
    "chunk_sizes = [256, 512, 1024, 2048]\n",
    "test_query = \"What are the benefits of attention mechanisms?\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Create splitter\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.2)  # 20% overlap\n",
    "    )\n",
    "    \n",
    "    # Process and index\n",
    "    temp_nodes = splitter.get_nodes_from_documents(sample_papers)\n",
    "    temp_index = VectorStoreIndex.from_documents(\n",
    "        sample_papers,\n",
    "        transformations=[splitter],\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    # Query\n",
    "    temp_engine = temp_index.as_query_engine(similarity_top_k=2)\n",
    "    temp_response = temp_engine.query(test_query)\n",
    "    \n",
    "    results.append({\n",
    "        \"Chunk Size\": chunk_size,\n",
    "        \"Num Nodes\": len(temp_nodes),\n",
    "        \"Top Score\": f\"{temp_response.source_nodes[0].score:.4f}\",\n",
    "        \"Response Len\": len(str(temp_response)),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nChunk Size Impact on Retrieval:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
