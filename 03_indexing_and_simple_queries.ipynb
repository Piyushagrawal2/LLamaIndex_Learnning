{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55475bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    Document,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.vector_stores import VectorStoreInfo, MetadataInfo\n",
    "from llama_index.core.retrievers import VectorIndexRetriever, VectorIndexAutoRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "try:\n",
    "    from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "    QDRANT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    QDRANT_AVAILABLE = False\n",
    "    print(\"⚠️  Qdrant not installed. Install with: pip install llama-index-vector-stores-qdrant qdrant-client\")\n",
    "\n",
    "#Embeddings\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "#LLM\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "#External Liberaries\n",
    "import chromadb\n",
    "if QDRANT_AVAILABLE:\n",
    "    from qdrant_client import QdrantClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c55d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Settings configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment and configure Settings\n",
    "load_dotenv()\n",
    "\n",
    "Settings.llm = GoogleGenAI(model=\"gemini-2.5-flash\", temperature=0.1)\n",
    "Settings.embed_model = GoogleGenAIEmbedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    dimensions=1536\n",
    ")\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 200\n",
    "\n",
    "print(\"✅ Settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d8cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 5 sample documents\n",
      "   Topics: vector_databases, chroma, qdrant, embeddings, algorithms\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive sample documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Vector databases are specialized databases designed to store and query high-dimensional vectors.\n",
    "        These vectors typically represent embeddings of text, images, or other data. Vector databases\n",
    "        enable efficient similarity search using algorithms like HNSW (Hierarchical Navigable Small World)\n",
    "        or IVF (Inverted File Index). Popular vector databases include Qdrant, Pinecone, Weaviate, and Milvus.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"vector_databases\", \"difficulty\": \"intermediate\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor\n",
    "        search. It builds a multi-layer graph where each layer is a subset of the previous one. The algorithm\n",
    "        achieves excellent query performance (sub-millisecond) with high recall. HNSW parameters include\n",
    "        M (number of connections per node) and ef_construction (search width during construction).\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"algorithms\", \"difficulty\": \"advanced\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Embedding models convert text into dense vector representations that capture semantic meaning.\n",
    "        OpenAI's text-embedding-3-small produces 1536-dimensional vectors and is optimized for retrieval tasks.\n",
    "        Open-source alternatives include sentence-transformers models like all-MiniLM-L6-v2 (384 dimensions)\n",
    "        and all-mpnet-base-v2 (768 dimensions). The choice of embedding model affects retrieval quality and cost.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"embeddings\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Qdrant is an open-source vector database written in Rust. It supports HNSW indexing, filtering,\n",
    "        and hybrid search. Qdrant can run locally (Docker) or in the cloud. Key features include payload\n",
    "        filtering, quantization for memory reduction, and distributed deployments. Qdrant is particularly\n",
    "        well-suited for production RAG applications.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"qdrant\", \"difficulty\": \"intermediate\", \"year\": 2024}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Chroma is a lightweight, embedded vector database designed for AI applications. It runs in-memory\n",
    "        or can persist to disk. Chroma is easy to set up and integrates seamlessly with LangChain and LlamaIndex.\n",
    "        It's ideal for prototyping and small-to-medium scale applications. Chroma supports metadata filtering\n",
    "        and multiple distance metrics (cosine, euclidean, dot product).\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"chroma\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(documents)} sample documents\")\n",
    "print(f\"   Topics: {', '.join(set(d.metadata['topic'] for d in documents))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944deef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. In-Memory Vector Store (Default)\n",
    "\n",
    "### SimpleVectorStore: LlamaIndex's Built-in Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d6423",
   "metadata": {},
   "source": [
    "## This is only for POC's, NOBODY USE THIS IN PRODUCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f618a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex (in-memory)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339d2bf76b984ec7825e5515d2a3485d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fea744c17e400385e74c0eb7842d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Index created in 0.66 seconds\n",
      "   Vector store type: SimpleVectorStore (in-memory)\n"
     ]
    }
   ],
   "source": [
    "# Create index with default in-memory vector store\n",
    "print(\"Creating VectorStoreIndex (in-memory)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "simple_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Index created in {elapsed:.2f} seconds\")\n",
    "print(f\"   Vector store type: SimpleVectorStore (in-memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5e039",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Chroma Vector Store Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849a2180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex with Chroma...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d038f4fb9f4ed3a101bc5a5ed4391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16970737a25243a0914dff251eacaeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Chroma index created in 0.83 seconds\n",
      "   Collection: llama_index_docs\n",
      "   Documents indexed: 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma client (in-memory)\n",
    "chroma_client = chromadb.EphemeralClient() #In-memory\n",
    "# For persistence: chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "#collection name\n",
    "collection_name = \"llama_index_docs\"\n",
    "chroma_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "# Create Chroma vector store\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)\n",
    "\n",
    "print(\"Creating VectorStoreIndex with Chroma...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chroma_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Chroma index created in {elapsed:.2f} seconds\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Documents indexed: {chroma_collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694ee82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Qdrant Vector Store Integration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce3fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex with Qdrant...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6b23d5c1514b5fb1dca61926a4a5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44ebf71ea8b4658b04e191c69f1a871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Qdrant index created in 0.52 seconds\n",
      "   Collection: llama_index_qdrant\n"
     ]
    }
   ],
   "source": [
    "if QDRANT_AVAILABLE:\n",
    "    # Initialize Qdrant client (in-memory)\n",
    "    qdrant_client = QdrantClient(location=\":memory:\")\n",
    "    # For persistence: QdrantClient(path=\"./qdrant_db\")\n",
    "    # For cloud: QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
    "    \n",
    "    # Create Qdrant vector store\n",
    "    qdrant_vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"llama_index_qdrant\",\n",
    "    )\n",
    "    \n",
    "    # Create storage context\n",
    "    qdrant_storage_context = StorageContext.from_defaults(vector_store=qdrant_vector_store)\n",
    "    \n",
    "    print(\"Creating VectorStoreIndex with Qdrant...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    qdrant_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=qdrant_storage_context,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Qdrant index created in {elapsed:.2f} seconds\")\n",
    "    print(f\"   Collection: llama_index_qdrant\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping Qdrant example (not installed)\")\n",
    "    qdrant_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8476a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleGenAI Embedding (gemini-embedding-001):\n",
      "  Dimensions: 768\n",
      "  Time: 356.02ms\n",
      "  First 5 values: [-0.01889738, 0.00028500613, -0.027077155, 0.00047685512, 0.001233422]\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI embedding\n",
    "openai_embed = GoogleGenAIEmbedding(\n",
    "    model=\"gemini-embedding-001\",\n",
    "    dimensions=1536,\n",
    ")\n",
    "\n",
    "test_text = \"Vector databases enable semantic search\"\n",
    "start_time = time.time()\n",
    "gemini_vector = openai_embed.get_text_embedding(test_text)\n",
    "gemini_time = time.time() - start_time\n",
    "\n",
    "print(f\"GoogleGenAI Embedding (gemini-embedding-001):\")\n",
    "print(f\"  Dimensions: {len(gemini_vector)}\")\n",
    "print(f\"  Time: {gemini_time*1000:.2f}ms\")\n",
    "print(f\"  First 5 values: {gemini_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fb460",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Index Persistence\n",
    "\n",
    "### Saving Index to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d9d5cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting index to ./storage...\n",
      "\n",
      "✅ Index persisted successfully!\n",
      "   Location: ./storage\n",
      "   Files created: 5\n",
      "     - image__vector_store.json\n",
      "     - graph_store.json\n",
      "     - index_store.json\n",
      "     - docstore.json\n",
      "     - default__vector_store.json\n"
     ]
    }
   ],
   "source": [
    "# Save index to disk\n",
    "persist_dir = \"./storage\"\n",
    "\n",
    "print(f\"Persisting index to {persist_dir}...\")\n",
    "simple_index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "print(\"\\n✅ Index persisted successfully!\")\n",
    "print(f\"   Location: {persist_dir}\")\n",
    "\n",
    "# Check what was saved\n",
    "storage_path = Path(persist_dir)\n",
    "if storage_path.exists():\n",
    "    files = list(storage_path.glob(\"*\"))\n",
    "    print(f\"   Files created: {len(files)}\")\n",
    "    for f in files:\n",
    "        print(f\"     - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954508a2",
   "metadata": {},
   "source": [
    "### Loading Index from Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02645c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from ./storage...\n",
      "Index loaded successfully.\n",
      "\n",
      "Test query on loaded index:\n",
      "  Query: What is Qdrant?\n",
      "  Response: Qdrant is an open-source vector database developed in Rust. It is designed to store and query high-dimensional vectors, supporting features such as HNSW indexing, filtering, and hybrid search. It can be deployed locally using Docker or in cloud environments. Its key capabilities include payload filtering, quantization for memory optimization, and distributed deployments, making it particularly suitable for production RAG applications.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Load index from disk\n",
    "print(f\"Loading index from {persist_dir}...\")\n",
    "\n",
    "storage_context_load = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "loaded_index = load_index_from_storage(storage_context_load)\n",
    "\n",
    "print(\"Index loaded successfully.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Async query execution\n",
    "# -----------------------------\n",
    "async def run_engine():\n",
    "    test_query_engine = loaded_index.as_query_engine(similarity_top_k=2)\n",
    "    test_response = await test_query_engine.aquery(\"what is Qdrant?\")\n",
    "\n",
    "    print(\"\\nTest query on loaded index:\")\n",
    "    print(\"  Query: What is Qdrant?\")\n",
    "    print(f\"  Response: {test_response}\")\n",
    "\n",
    "# In Jupyter / Notebook\n",
    "await run_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c63e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Query Engine Configuration\n",
    "\n",
    "### 8.1 Basic Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94f1c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main vector databases mentioned?\n",
      "\n",
      "Response:\n",
      "The main vector databases mentioned are Qdrant, Pinecone, Weaviate, and Milvus.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sources used: 2\n"
     ]
    }
   ],
   "source": [
    "query_engine = chroma_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "async def run_query(query_text: str):\n",
    "    response = await query_engine.aquery(query_text)\n",
    "\n",
    "    print(f\"Query: {query_text}\\n\")\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"\\nSources used: {len(response.source_nodes)}\")\n",
    "\n",
    "# Jupyter / Notebook\n",
    "await run_query(\"What are the main vector databases mentioned?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47161350",
   "metadata": {},
   "source": [
    "### 8.2 Response Synthesis Modes Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing response modes with query: 'Explain HNSW algorithm'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mode: compact\n",
      "  Time: 2.60s\n",
      "  Response length: 604 chars\n",
      "  Response preview: HNSW (Hierarchical Navigable Small World) is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph where each successive layer is a subset of the ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: tree_summarize\n",
      "  Time: 2.63s\n",
      "  Response length: 621 chars\n",
      "  Response preview: HNSW (Hierarchical Navigable Small World) is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph where each successive layer is a subset of the ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: simple_summarize\n",
      "  Time: 2.49s\n",
      "  Response length: 622 chars\n",
      "  Response preview: HNSW (Hierarchical Navigable Small World) is a graph-based algorithm designed for approximate nearest neighbor search. It constructs a multi-layer graph where each successive layer is a subset of the ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: refine\n",
      "  Time: 6.61s\n",
      "  Response length: 512 chars\n",
      "  Response preview: HNSW is a graph-based algorithm designed for approximate nearest neighbor search. It operates by constructing a multi-layer graph, where each successive layer is a subset of the one before it. This ap...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "modes = [\"compact\", \"tree_summarize\", \"simple_summarize\", \"refine\"]\n",
    "test_query = \"Explain HNSW algorithm\"\n",
    "\n",
    "print(f\"Testing response modes with query: '{test_query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    " ync def test_response_modes():\n",
    "    for mode in modes:\n",
    "        engine = chroma_index.as_query_engine(\n",
    "            response_mode=mode,\n",
    "            similarity_top_k=2,\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        response = await engine.aquery(test_query)\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print(f\"\\nMode: {mode}\")\n",
    "        print(f\"  Time: {elapsed:.2f}s\")\n",
    "        print(f\"  Response length: {len(str(response))} chars\")\n",
    "        print(f\"  Response preview: {str(response)[:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Jupyter / Notebook\n",
    "await test_response_modes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce720da",
   "metadata": {},
   "source": [
    "### 8.3 Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54f40f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the difference between Qdrant and Chroma?\n",
      "\n",
      "Streaming response:\n",
      "--------------------------------------------------------------------------------\n",
      "Qdrant is an open-source vector database written in Rust, designed for production RAG applications, and supports distributed deployments, HNSW indexing, hybrid search, and quantization for memory reduction. It can be run locally via Docker or in the cloud.\n",
      "\n",
      "Chroma, on the other hand, is a lightweight, embedded vector database primarily for AI applications, ideal for prototyping and small-to-medium scale use cases. It runs in-memory or can persist to disk, is easy to set up, and integrates well with LangChain and LlamaIndex, offering metadata filtering and various distance metrics.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create streaming query engine\n",
    "streaming_engine = chroma_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "query_text = \"What is the difference between Qdrant and Chroma?\"\n",
    "print(f\"Query: {query_text}\\n\")\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "async def run_streaming_query():\n",
    "    response = await streaming_engine.aquery(query_text)\n",
    "\n",
    "    # ✅ async generator → async for\n",
    "    async for text in response.response_gen:\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Jupyter / Notebook\n",
    "await run_streaming_query()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
